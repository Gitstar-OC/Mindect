---
title: Polynomial Regression
---

## Introduction to Polynomial Regression

So far, we've primarily been fitting straight lines to our data. Let's take the ideas of multiple linear regression and feature engineering to develop a new algorithm called `polynomial regression`, which allows you to fit curves, or non-linear functions, to your data.

## Example of Housing Data

Consider a housing dataset that looks like this:

![FEPR6](/SL/GDP/FEPR6.png)

Here, feature \( x \) represents the size in square feet. A straight line doesn't appear to fit this dataset very well.

### Fitting a Quadratic Function

You might want to fit a curve, perhaps a quadratic function, to the data, like this:

![FEPR1](/SL/GDP/FEPR1.png)

This function includes size \( x \) and also \( x^2 \), which is the size raised to the power of two. This could potentially provide a better fit to the data.

### Choosing a Cubic Function

However, you may find that a quadratic model doesn't make sense because a quadratic function eventually decreases. We wouldn't expect housing prices to drop as size increases. 

Thus, you may choose a cubic function that includes not only \( x^2 \) but also \( x^3 \):

![FEPR3](/SL/GDP/FEPR3.png)

This model generates a curve that fits the data better since the size does eventually increase as the size grows. These are examples of polynomial regression, where you take the original feature \( x \) and raise it to different powers, such as two or three.

## Importance of Feature Scaling

It’s essential to note that when you create features like these powers (e.g., the square or cube of the original features), feature scaling becomes increasingly important. 

For example, if the size of the house ranges from 1 to 1,000 square feet, then:
- The second feature, size squared, will range from 1 to 1,000,000.
- The third feature, size cubed, will range from 1 to 1,000,000,000.

![FEPR4](/SL/GDP/FEPR4.png)

These features, \( x^2 \) and \( x^3 \), take on very different value ranges compared to the original feature \( x \). `When using gradient descent, it's crucial to apply feature scaling to ensure your features are within comparable ranges.`

## Alternative Features: Square Root

As an alternative to using the size squared and cubed, you could consider using the square root of \( x \):

![FEPR5](/SL/GDP/FEPR5.png)

In this case, your model might look like
 
 ```math
  w_1 \cdot x + w_2 \cdot \sqrt{x} + b
 ```
 The square root function becomes less steep as \( x \) increases but never completely flattens out and does not decrease. This could be another viable feature choice for this dataset.

## Choosing Features

You may wonder how to decide which features to use. In the notes on `Advanced Algorithm`, you will learn how to select different features and models, including which features to include or exclude. A systematic approach for measuring the performance of these models will also be discussed, aiding your feature selection process.

## Conclusion on Feature Engineering

For now, it's essential to recognize that you have the flexibility to choose your features. By utilizing feature engineering and polynomial functions, you can develop a more accurate model for your data.

In the upcoming notebook, you'll see code that implements polynomial regression using features like \( x \), \( x^2 \), and \( x^3 \). Please review and run the code to observe how it functions. 

There is also another notebook that demonstrates how to use `Scikit-learn`, a widely-used open-source machine learning library utilized by many practitioners in top AI and machine learning companies.

If you are using machine learning in your job, there’s a good chance you will utilize tools like `Scikit-learn` to train your models. Working through that notebook will enhance your understanding of linear regression and show how it can be executed in just a few lines of code using a library like `Scikit-learn`.

To gain a solid grasp of these algorithms and their applications, it’s crucial to understand how to implement linear regression independently, rather than merely relying on a `Scikit-learn` function that acts as a black box. Nonetheless, `Scikit-learn` plays a significant role in contemporary machine learning practices.

Congratulations on completing this section! Please explore the practice quizzes and the practice lab, where you can apply the concepts we've discussed. In this week's practice lab, you'll implement linear regression. I hope you enjoy the process of getting this learning algorithm to work for you. Best of luck with that! 

In the next section, we will move beyond regression—predicting numerical values—to discuss our first classification algorithm, which can predict categories.

{/*
---
title: Polynomial Regression
---


So far we've just been fitting straight lines to our data. Let's take the ideas of multiple linear regression and feature engineering to come up with a new algorithm called `polynomial regression`, `which will let you fit curves, non-linear functions, to your data`.

Let's say you have a housing data-set that looks like this
![FEPR6](/SL/GDP/FEPR6.png)

where feature x is the size in square feet. It doesn't look like a straight line fits this data-set very well.

Maybe you want to fit a curve, maybe a quadratic function to the data like this

![FEPR1](/SL/GDP/FEPR1.png)

which includes a size x and also x squared, which is the size raised to the power of two. Maybe that will give you a better fit to the data.

But then you may decide that your quadratic model doesn't really make sense because a quadratic function eventually comes back down. Well, we wouldn't really expect housing prices to go down when the size increases. Big houses seem like they should usually cost more.

Then you may choose a cubic function where we now have not only x squared, but x cubed.
![FEPR3](/SL/GDP/FEPR3.png)
Maybe this model produces this curve here, which is a somewhat better fit to the data because the size does eventually come back up as the size increases. These are both examples of polynomial regression, because you took your optional feature x, and raised it to the power of two or three or any other power. In the case of the cubic function, the first feature is the size, the second feature is the size squared, and the third feature is the size cubed.

I just want to point out one more thing, which is that if you create features that are these powers like the square of the original features like this, then feature scaling becomes increasingly important. If the size of the house ranges from say, 1-1,000 square feet, then the second feature, which is a size squared, will range from one to a million, and the third feature, which is size cubed, ranges from one to a billion.

![FEPR4](/SL/GDP/FEPR4.png)

These two features, x squared and x cubed, take on very different ranges of values compared to the original feature x. `If you're using gradient descent, it's important to apply feature scaling to get your features into comparable ranges of values.

Finally, just one last example of how you really have a wide range of choices of features to use. Another reasonable alternative to taking the size squared and size cubed is to say use the square root of x.

![FEPR5](/SL/GDP/FEPR5.png)

Your model may look like w_1 times x plus w_2 times the square root of x plus b. The square root function looks like this, and it becomes a bit less steep as x increases, but it doesn't ever completely flatten out, and it certainly never ever comes back down. This would be another choice of features that might work well for this data-set as well. You may ask yourself, how do I decide what features to use? Later in the notes of `Advanced Algorithm`, you will see how you can choose different features and different models that include or don't include these features, and you have a process for measuring how well these different models perform to help you decide which features to include or not include. 

For now, I just want you to be aware that you have a choice in what features you use. By using feature engineering and polynomial functions, you can potentially get a much better model for your data. 

In the notebook that follows this part, you will see some code that implements polynomial regression using features like x, x squared, and x cubed. Please take a look and run the code and see how it works. There's also another notebook after that one that shows how to use a popular open source toolkit that implements linear regression. `Scikit-learn` is a very widely used open source machine learning library that is used by many practitioners in many of the top AI, internet, machine learning companies in the world.

If either now or in the future you're using machine learning in your job, there's a very good chance you'll be using tools like `Scikit-learn` to train your models. Working through that notebook will give you a chance to not only better understand linear regression, but also see how this can be done in just a few lines of code using a library like `Scikit-learn`. For you to have a solid understanding of these algorithms, and be able to apply them, I do think is important that you know how to implement linear regression yourself and not just call some `scikit-learn` function that is a black-box. But `scikit-learn also has an important role in a way machine learning is done in practice today`. 

Congratulations on finishing this section. Please do take a look at the practice quizzes and also the practice lab, which I hope will let you try out and practice ideas that we've discussed. In this week's practice lab, you implement linear regression. I hope you have a lot of fun getting this learning algorithm to work for yourself. Best of luck with that. In the next section we'll go beyond regression, that is predicting numbers, to talk about our first classification algorithm, which can predict categories.
*/}