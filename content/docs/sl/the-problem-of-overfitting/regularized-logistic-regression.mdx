---
title: Regularized Logistic Regression
---

## Introduction

In this part, we will see how to implement `regularized logistic regression`. Similar to regularized linear regression, we will modify logistic regression to prevent overfitting. Overfitting happens when the model is too complex and captures noise in the data, which leads to poor generalization to unseen examples.

## Logistic Regression and Overfitting

We saw earlier that logistic regression can be prone to overfitting, especially when using high-order polynomial features. Let’s take a closer look:

![RLOR (1)](/SL/PO/RLOR%20(1).png)

In particular, fitting logistic regression with many features can lead to a complex decision boundary, which risks overfitting the training set. A simpler decision boundary that generalizes better is preferable.

![RLOR (2)](/SL/PO/RLOR%20(2).png)

## Regularized Cost Function

We modify the cost function for logistic regression by adding a regularization term:

```math
J(w, b) = -\frac{1}{m} \sum_{i=1}^{m} \left[y^{(i)} \log(f^{(i)}) + (1 - y^{(i)}) \log(1 - f^{(i)}) \right] + \frac{\lambda}{2m} \sum_{j=1}^{n} w_j^2
```

Here, `λ` is the regularization parameter. Regularization discourages large weights by penalizing them in the cost function.

![RLOR (3)](/SL/PO/RLOR%20(3).png)

Minimizing this regularized cost function prevents parameters `w_j` from becoming too large, allowing for better generalization to new examples.

![RLOR (4)](/SL/PO/RLOR%20(4).png)

## Gradient Descent for Regularized Logistic Regression

Just like in regularized linear regression, we can update the weights and bias using gradient descent:

```math
w_j := w_j - \alpha \left( \frac{1}{m} \sum_{i=1}^{m} (f^{(i)} - y^{(i)}) x_j^{(i)} + \frac{\lambda}{m} w_j \right)
```

```math
b := b - \alpha \left( \frac{1}{m} \sum_{i=1}^{m} (f^{(i)} - y^{(i)}) \right)
```

Note that the update rule for `b` remains unchanged, as we do not regularize `b`.

![RLOR (5)](/SL/PO/RLOR%20(5).png)

## Implementing Regularized Logistic Regression

To implement regularized logistic regression, you apply the gradient descent update rules above. The only difference between regularized linear and logistic regression is that the prediction function `f` is now the sigmoid of `z`, where:

```math
f^{(i)} = \frac{1}{1 + e^{-z^{(i)}}}
```

![RLOR (6)](/SL/PO/RLOR%20(6).png)

## Intuition for Regularization

The regularization term shrinks the weights `w_j` by a small amount on every iteration of gradient descent, which helps prevent overfitting.

```math
w_j := w_j \cdot (1 - \alpha \frac{\lambda}{m}) - \alpha \cdot \left( \text{gradient term} \right)
```

This helps logistic regression generalize better to new examples.

![RLOR (7)](/SL/PO/RLOR%20(7).png)

## Final Thoughts

By now, you’ve learned how to implement regularized logistic regression to reduce overfitting, even with a large number of features. You should practice this in the upcoming labs and apply regularization to avoid overfitting in logistic regression.

![RLOR (8)](/SL/PO/RLOR%20(8).png)

Congratulations on reaching the end of this section! There’s much more to learn, and in the next part, we’ll explore **Neural Networks** and their fascinating applications in **Deep Learning**.

  
{/*
---
title: Regularized Logistic Regression
---


In this parts, you see how to implement `regularized logistic regression`. Just as the gradient update for logistic regression has seemed surprisingly similar to the gradient update for linear regression, you find that the gradient descent update for regularized logistic regression will also look similar to the update for regularized linear regression. Let's take a look.

Here is the idea. We saw earlier that logistic regression can be prone to overfitting if you fit it with very high order polynomial features like this. Here, z is a high order polynomial that gets passed into the sigmoid function like so to compute f.

![RLOR (1)](/SL/PO/RLOR%20(1).png)

In particular, you can end up with a decision boundary that is overly complex and overfits as training set. More generally, when you train logistic regression with a lot of features, whether polynomial features or some other features, there could be a higher risk of overfitting.

![RLOR (2)](/SL/PO/RLOR%20(2).png)

This was the cost function for logistic regression. If you want to modify it to use regularization, all you need to do is add to it the following term.

Let's add lambda to regularization parameter over 2m times the sum from j equals 1 through n, where n is the number of features as usual of wj squared.

![RLOR (3)](/SL/PO/RLOR%20(3).png)

When you minimize this cost function as a function of w and b, it has the effect of penalizing parameters w_1, w_2 through w_n, and preventing them from being too large. If you do this, then even though you're fitting a high order polynomial with a lot of parameters, you still get a decision boundary that looks like this.

![RLOR (4)](/SL/PO/RLOR%20(4).png)

Something that looks more reasonable for separating positive and negative examples while also generalizing hopefully to new examples not in the training set.

When using regularization, even when you have a lot of features. How can you actually implement this? How can you actually minimize this cost function j of wb that includes the regularization term? Well, let's use gradient descent as before. Here's a cost function that you want to minimize.

![RLOR (5)](/SL/PO/RLOR%20(5).png)

To implement gradient descent, as before, we'll carry out the following simultaneous updates over wj and b. These are the usual update rules for gradient descent.

![RLOR (6)](/SL/PO/RLOR%20(6).png)

Just like regularized linear regression, when you compute where there are these derivative terms, the only thing that changes now is that the derivative respect to wj gets this additional term, lambda over m times wj added here at the end. 

![RLOR (7)](/SL/PO/RLOR%20(7).png)

Again, it looks a lot like the update for `regularized linear regression`. In fact is the exact same equation, except for the fact that the definition of f is now no longer the linear function, it is the logistic function applied to z. 

![RLOR (8)](/SL/PO/RLOR%20(8).png)

Similar to linear regression, `we will regularize only the parameters w, j, but not the parameter b, which is why there's no change the update you will make for b`.

In the final notebook of this week, you revisit overfitting. In the interactive plot in the notebook, you can now choose to regularize your models, both regression and classification, by enabling regularization during gradient descent by selecting a value for lambda. Please take a look at the code for implementing regularized logistic regression in particular, because you'll implement this in practice lab yourself at the end of this week. Now you know how to implement regularized logistic regression. When I walk around Silicon Valley, there are many engineers using machine learning to create a ton of value, sometimes making a lot of money for the companies. I know you've only been studying this stuff for a few weeks but if you understand and can apply linear regression and logistic regression, that's actually all you need to create some very valuable applications. While the specific learning outcomes you use are important, knowing things like when and how to reduce overfitting turns out to be one of the very valuable skills in the real world as well. 

I want to say congratulations on how far you've come and I want to say great job for getting through all the way to the end of this parts I hope you also work through the practice labs. Having said that, there are still many more exciting things to learn. In the next section of this specialization, you'll learn about neural networks, also called deep learning algorithms. Neural networks are responsible for many of the latest breakthroughs in the eye today, from practical speech recognition to computers accurately recognizing objects and images, to self-driving cars. The way neural network gets built actually uses a lot of what you've already learned, like cost functions, and gradient descent, and sigmoid functions. 

Again, congratulations on reaching the end of this supervised machine learning . I hope you have fun in the noteboks and I will see you in Advanced Algorithms.
*/}

